# -*- coding: utf-8 -*-
"""
ì¹´í…Œê³ ë¦¬ 'ëŒ€í•œë¯¼êµ­ì˜ ë²”ì£„ ë“œë¼ë§ˆ'ì—ì„œ ëª¨ë“  ì»¬ëŸ¼(ã„±/ã„´/ã„· â€¦) + í˜ì´ì§€ë„¤ì´ì…˜ê¹Œì§€ í›‘ì–´ì„œ
ê° í•­ëª© ìƒì„¸ í˜ì´ì§€ì˜ 'ì œëª© / ì¥ë¥´ / ë°©ì†¡ì‚¬'ë§Œ ì¶”ì¶œí•˜ì—¬ CSV ì €ì¥.
ì €ì¥ ì „ì— genre_name ìë™ ë³´ì •:
  1) ë¹„ì—ˆê±°ë‚˜ ê²°ì¸¡ â†’ "ë²”ì£„"
  2) "ë²”ì£„" ë¯¸í¬í•¨ â†’ "ë²”ì£„, {ê¸°ì¡´ì¥ë¥´}"

ì¶œë ¥: crime_dramas_all.csv (UTF-8 with BOM), ì»¬ëŸ¼: title, genre_name, channel_name
"""

import re
import time
from urllib.parse import urljoin
from typing import Optional

import pandas as pd
import requests
from bs4 import BeautifulSoup

BASE = "https://ko.wikipedia.org"
CATEGORY_URL = "https://ko.wikipedia.org/wiki/%EB%B6%84%EB%A5%98:%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD%EC%9D%98_%EB%B2%94%EC%A3%84_%EB%93%9C%EB%9D%BC%EB%A7%88"

HEADERS = {
    "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) "
                   "Chrome/122.0.0.0 Safari/537.36"),
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.7,en;q=0.5",
}

SLEEP = 0.7  # ìš”ì²­ ê°„ ë§¤ë„ˆ íƒ€ì„(ì´ˆ)
NEXT_SELECTOR = "#mw-pages > a:nth-child(3)"  # ìš°ì„  ì‚¬ìš©ë  'ë‹¤ìŒ í˜ì´ì§€' CSS ì„ íƒì
GENRE_KEYWORD = "ë²”ì£„"  # ì´ ìŠ¤í¬ë¦½íŠ¸ì˜ ì¥ë¥´ í‚¤ì›Œë“œ

def clean_text(s: str) -> str:
    if not s:
        return ""
    s = re.sub(r"\[[^\]]*\]", "", s)  # [ì£¼ 1], [1] ë“± ì œê±°
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def get_soup(url: str) -> BeautifulSoup:
    r = requests.get(url, headers=HEADERS, timeout=30)
    r.raise_for_status()
    return BeautifulSoup(r.text, "lxml")

def find_next_link(soup: BeautifulSoup) -> Optional[str]:
    """ì§€ì • ì…€ë ‰í„° ìš°ì„ , ì‹¤íŒ¨ ì‹œ 'ë‹¤ìŒ í˜ì´ì§€' í…ìŠ¤íŠ¸ í´ë°±."""
    # 1) ì§€ì • ì…€ë ‰í„° ì‹œë„
    el = soup.select_one(NEXT_SELECTOR)
    if el and el.get("href"):
        return urljoin(BASE, el.get("href"))

    # 2) í´ë°±: í…ìŠ¤íŠ¸ê°€ 'ë‹¤ìŒ í˜ì´ì§€'ì¸ ë§í¬
    for a in soup.select("#mw-pages a"):
        if clean_text(a.get_text()) == "ë‹¤ìŒ í˜ì´ì§€" and a.get("href"):
            return urljoin(BASE, a.get("href"))
    return None

def iter_all_category_links(first_url: str):
    """
    ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ì—ì„œ:
      - ì»¬ëŸ¼ ì¸ë±ìŠ¤ 1ë¶€í„° ì¦ê°€ì‹œí‚¤ë©° ì¡´ì¬í•  ë•Œê¹Œì§€(ì—†ìœ¼ë©´ ì¢…ë£Œ)
      - ê° ì»¬ëŸ¼ì˜ ëª¨ë“  <li><a>ë¥¼ ìˆ˜ì§‘
      - 'ë‹¤ìŒ í˜ì´ì§€'ê°€ ìˆìœ¼ë©´ ì§€ì • ì…€ë ‰í„°/í´ë°±ìœ¼ë¡œ ë”°ë¼ê°€ë©° ë°˜ë³µ
    """
    url = first_url
    seen_urls = set()

    while url and url not in seen_urls:
        seen_urls.add(url)
        soup = get_soup(url)

        col_idx = 1
        found_any = False
        while True:
            col_sel = f"#mw-pages > div > div > div:nth-child({col_idx})"
            col = soup.select_one(col_sel)
            if not col:
                break

            for a in col.select("ul > li > a"):
                href = a.get("href")
                if not href:
                    continue
                yield urljoin(BASE, href)
                found_any = True

            col_idx += 1

        if not found_any:
            break

        # 'ë‹¤ìŒ í˜ì´ì§€' (ì…€ë ‰í„° ìš°ì„  + í´ë°±)
        url = find_next_link(soup)
        time.sleep(SLEEP)

def scrape_detail(detail_url: str) -> dict:
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ 'ì œëª© / ì¥ë¥´ / ë°©ì†¡ì‚¬' ì¶”ì¶œ.
    - ì œëª©: h1#firstHeading
    - ì¥ë¥´/ë°©ì†¡ì‚¬: infoboxì˜ th ë¼ë²¨ í™•ì¸
    """
    soup = get_soup(detail_url)

    # ì œëª©
    title_el = soup.select_one("#firstHeading")
    title = clean_text(title_el.get_text()) if title_el else ""

    # infobox
    infobox = soup.select_one("#mw-content-text table.infobox")
    genre = ""
    broadcaster = ""

    if infobox:
        for tr in infobox.select("tr"):
            th = tr.find("th")
            td = tr.find("td")
            if not td:
                continue
            label = clean_text(th.get_text()) if th else ""
            value_text = clean_text(td.get_text())

            # ì¥ë¥´
            if label and ("ì¥ë¥´" in label) and not genre:
                genre = value_text

            # ë°©ì†¡ì‚¬/ì±„ë„/ë°©ì†¡êµ­
            if label and re.search(r"(ë°©ì†¡\s*ì‚¬|ë°©ì†¡\s*ì±„ë„|ì±„ë„|ë°©ì†¡êµ­)", label) and not broadcaster:
                links = [clean_text(a.get_text()) for a in td.select("a") if clean_text(a.get_text())]
                broadcaster = "; ".join(links) if links else value_text

    return {
        "title": title,
        "genre_name": genre,
        "channel_name": broadcaster.replace(";", ", ").strip(", ").strip()
    }

def fix_genre_value(s: object, keyword: str = GENRE_KEYWORD) -> str:
    """
    ì¥ë¥´ ìë™ ë³´ì • ê·œì¹™:
      1) ê²°ì¸¡/ë¹ˆë¬¸ìì—´/ê³µë°±/ë¬¸ìì—´ 'nan'/'none' â†’ '{keyword}'
      2) '{keyword}' ë¯¸í¬í•¨ â†’ '{keyword}, {ê¸°ì¡´ì¥ë¥´}'
      3) ë¶ˆí•„ìš”í•œ ì¤‘ë³µ êµ¬ë‘ì /ê³µë°± ì •ë¦¬
    """
    if pd.isna(s):
        return keyword

    s = str(s).strip()
    if not s or s.lower() in {"nan", "none"}:
        return keyword

    s = s.strip(",; ")
    if keyword in s:
        return s

    fixed = f"{keyword}, {s}"
    fixed = re.sub(r"[;,]\s*[;,]+", ", ", fixed)  # ;;, ,, ê°™ì€ ì¤‘ë³µ êµ¬ë‘ì  ì •ë¦¬
    fixed = re.sub(r"\s*,\s*", ", ", fixed)      # ì‰¼í‘œ ì£¼ë³€ ê³µë°± ì •ë¦¬
    return fixed.strip(",; ").strip() or keyword

def main():
    print("[*] í¬ë¡¤ë§ ì‹œì‘:", CATEGORY_URL)
    rows = []
    seen = set()

    for idx, url in enumerate(iter_all_category_links(CATEGORY_URL), 1):
        if url in seen:
            continue
        seen.add(url)

        print(f"  ({idx}) {url}")
        try:
            row = scrape_detail(url)
            rows.append(row)
        except Exception as e:
            print(f"     - FAIL: {e}")
        time.sleep(SLEEP)

    if not rows:
        print("[-] ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # DataFrame êµ¬ì„± ë° ì •ë¦¬
    df = pd.DataFrame(rows, columns=["title", "genre_name", "channel_name"])
    for c in ["title", "genre_name", "channel_name"]:
        df[c] = df[c].astype(str).map(clean_text)

    # ì œëª© ê¸°ì¤€ ì¤‘ë³µ ì œê±°
    df = df.drop_duplicates(subset=["title"])

    # ğŸ”¹ ì¥ë¥´ ìë™ ë³´ì • (ë¹ˆê°’ â†’ "ë²”ì£„", ë¯¸í¬í•¨ ì‹œ ë§¨ ì•ì— "ë²”ì£„, " ì¶”ê°€)
    df["genre_name"] = df["genre_name"].apply(fix_genre_value)

    out = "crime_dramas_all.csv"
    df.to_csv(out, index=False, encoding="utf-8-sig")
    print(f"[âœ“] ì¥ë¥´ ë³´ì • í¬í•¨ ì €ì¥ ì™„ë£Œ: {out} (í–‰ ìˆ˜: {len(df)})")

if __name__ == "__main__":
    main()
